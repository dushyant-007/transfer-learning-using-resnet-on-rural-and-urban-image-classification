{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Getting started.\n",
    "\n",
    "Import all the libraries that are to be used.\n",
    "1. the pytorch framework library.\n",
    "2. torchvision will also be useful to show the arch of the network.\n",
    "3. resnet have to be import , Idk is it part of the torch library? gotta check.\n",
    "4. numpy\n",
    "5. pyplot\n",
    "\n",
    "I'll keep adding if I see something."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:19:22.055008900Z",
     "start_time": "2023-05-15T20:19:22.007697Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:19:23.095437100Z",
     "start_time": "2023-05-15T20:19:22.815488100Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%reload_ext autoreload\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsummary\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "from PIL import  Image\n",
    "from load_model import resnet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Alright , now lets try to extract resnet architecture and printout its arch using torchsummary"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            9,408\n",
      "├─BatchNorm2d: 1-2                       128\n",
      "├─ReLU: 1-3                              --\n",
      "├─MaxPool2d: 1-4                         --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─BasicBlock: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  36,864\n",
      "|    |    └─BatchNorm2d: 3-2             128\n",
      "|    |    └─ReLU: 3-3                    --\n",
      "|    |    └─Conv2d: 3-4                  36,864\n",
      "|    |    └─BatchNorm2d: 3-5             128\n",
      "|    └─BasicBlock: 2-2                   --\n",
      "|    |    └─Conv2d: 3-6                  36,864\n",
      "|    |    └─BatchNorm2d: 3-7             128\n",
      "|    |    └─ReLU: 3-8                    --\n",
      "|    |    └─Conv2d: 3-9                  36,864\n",
      "|    |    └─BatchNorm2d: 3-10            128\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─BasicBlock: 2-3                   --\n",
      "|    |    └─Conv2d: 3-11                 73,728\n",
      "|    |    └─BatchNorm2d: 3-12            256\n",
      "|    |    └─ReLU: 3-13                   --\n",
      "|    |    └─Conv2d: 3-14                 147,456\n",
      "|    |    └─BatchNorm2d: 3-15            256\n",
      "|    |    └─Sequential: 3-16             8,448\n",
      "|    └─BasicBlock: 2-4                   --\n",
      "|    |    └─Conv2d: 3-17                 147,456\n",
      "|    |    └─BatchNorm2d: 3-18            256\n",
      "|    |    └─ReLU: 3-19                   --\n",
      "|    |    └─Conv2d: 3-20                 147,456\n",
      "|    |    └─BatchNorm2d: 3-21            256\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─BasicBlock: 2-5                   --\n",
      "|    |    └─Conv2d: 3-22                 294,912\n",
      "|    |    └─BatchNorm2d: 3-23            512\n",
      "|    |    └─ReLU: 3-24                   --\n",
      "|    |    └─Conv2d: 3-25                 589,824\n",
      "|    |    └─BatchNorm2d: 3-26            512\n",
      "|    |    └─Sequential: 3-27             33,280\n",
      "|    └─BasicBlock: 2-6                   --\n",
      "|    |    └─Conv2d: 3-28                 589,824\n",
      "|    |    └─BatchNorm2d: 3-29            512\n",
      "|    |    └─ReLU: 3-30                   --\n",
      "|    |    └─Conv2d: 3-31                 589,824\n",
      "|    |    └─BatchNorm2d: 3-32            512\n",
      "├─Sequential: 1-8                        --\n",
      "|    └─BasicBlock: 2-7                   --\n",
      "|    |    └─Conv2d: 3-33                 1,179,648\n",
      "|    |    └─BatchNorm2d: 3-34            1,024\n",
      "|    |    └─ReLU: 3-35                   --\n",
      "|    |    └─Conv2d: 3-36                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-37            1,024\n",
      "|    |    └─Sequential: 3-38             132,096\n",
      "|    └─BasicBlock: 2-8                   --\n",
      "|    |    └─Conv2d: 3-39                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-40            1,024\n",
      "|    |    └─ReLU: 3-41                   --\n",
      "|    |    └─Conv2d: 3-42                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-43            1,024\n",
      "├─AdaptiveAvgPool2d: 1-9                 --\n",
      "├─Linear: 1-10                           1,026\n",
      "=================================================================\n",
      "Total params: 11,177,538\n",
      "Trainable params: 11,177,538\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Conv2d: 1-1                            9,408\n├─BatchNorm2d: 1-2                       128\n├─ReLU: 1-3                              --\n├─MaxPool2d: 1-4                         --\n├─Sequential: 1-5                        --\n|    └─BasicBlock: 2-1                   --\n|    |    └─Conv2d: 3-1                  36,864\n|    |    └─BatchNorm2d: 3-2             128\n|    |    └─ReLU: 3-3                    --\n|    |    └─Conv2d: 3-4                  36,864\n|    |    └─BatchNorm2d: 3-5             128\n|    └─BasicBlock: 2-2                   --\n|    |    └─Conv2d: 3-6                  36,864\n|    |    └─BatchNorm2d: 3-7             128\n|    |    └─ReLU: 3-8                    --\n|    |    └─Conv2d: 3-9                  36,864\n|    |    └─BatchNorm2d: 3-10            128\n├─Sequential: 1-6                        --\n|    └─BasicBlock: 2-3                   --\n|    |    └─Conv2d: 3-11                 73,728\n|    |    └─BatchNorm2d: 3-12            256\n|    |    └─ReLU: 3-13                   --\n|    |    └─Conv2d: 3-14                 147,456\n|    |    └─BatchNorm2d: 3-15            256\n|    |    └─Sequential: 3-16             8,448\n|    └─BasicBlock: 2-4                   --\n|    |    └─Conv2d: 3-17                 147,456\n|    |    └─BatchNorm2d: 3-18            256\n|    |    └─ReLU: 3-19                   --\n|    |    └─Conv2d: 3-20                 147,456\n|    |    └─BatchNorm2d: 3-21            256\n├─Sequential: 1-7                        --\n|    └─BasicBlock: 2-5                   --\n|    |    └─Conv2d: 3-22                 294,912\n|    |    └─BatchNorm2d: 3-23            512\n|    |    └─ReLU: 3-24                   --\n|    |    └─Conv2d: 3-25                 589,824\n|    |    └─BatchNorm2d: 3-26            512\n|    |    └─Sequential: 3-27             33,280\n|    └─BasicBlock: 2-6                   --\n|    |    └─Conv2d: 3-28                 589,824\n|    |    └─BatchNorm2d: 3-29            512\n|    |    └─ReLU: 3-30                   --\n|    |    └─Conv2d: 3-31                 589,824\n|    |    └─BatchNorm2d: 3-32            512\n├─Sequential: 1-8                        --\n|    └─BasicBlock: 2-7                   --\n|    |    └─Conv2d: 3-33                 1,179,648\n|    |    └─BatchNorm2d: 3-34            1,024\n|    |    └─ReLU: 3-35                   --\n|    |    └─Conv2d: 3-36                 2,359,296\n|    |    └─BatchNorm2d: 3-37            1,024\n|    |    └─Sequential: 3-38             132,096\n|    └─BasicBlock: 2-8                   --\n|    |    └─Conv2d: 3-39                 2,359,296\n|    |    └─BatchNorm2d: 3-40            1,024\n|    |    └─ReLU: 3-41                   --\n|    |    └─Conv2d: 3-42                 2,359,296\n|    |    └─BatchNorm2d: 3-43            1,024\n├─AdaptiveAvgPool2d: 1-9                 --\n├─Linear: 1-10                           1,026\n=================================================================\nTotal params: 11,177,538\nTrainable params: 11,177,538\nNon-trainable params: 0\n================================================================="
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_classes = 2\n",
    "model = models.resnet18(pretrained= True)\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features , out_features=num_classes)\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "model.fc.requires_grad_(True)\n",
    "\n",
    "torchsummary.summary(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:19:32.533762400Z",
     "start_time": "2023-05-15T20:19:32.072588700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:19:34.497906900Z",
     "start_time": "2023-05-15T20:19:34.403422300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "let's get some small subset of images from Imagenet to test the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "now that we have the images, they are person, cat ,  dog , horse, car (in that order.), let's run resnet on it to see how it performs.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:19:41.166337800Z",
     "start_time": "2023-05-15T20:19:41.121026200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Now , let's get access to the r/u data. we will have a data_loader that will throw the data in batches at a time. let's take the batch size as 8 so that we can work with the dataloaders."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from load_data import get_data\n",
    "train_loader, test_loader = get_data(batch_size =32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:19:43.200096Z",
     "start_time": "2023-05-15T20:19:43.174710600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "opt = Adam(model.parameters() , lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:19:43.688171500Z",
     "start_time": "2023-05-15T20:19:43.664374200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:19:44.713860400Z",
     "start_time": "2023-05-15T20:19:44.690176Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at epoch 0 is 0.0018436063546687365\n",
      "pred = tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0]) \n",
      " label= tensor([1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0])\n",
      "\n",
      "test_loss at epoch 0 is 0.08725837618112564\n",
      "\n",
      "training loss at epoch 1 is 0.005015980452299118\n",
      "test_loss at epoch 1 is 0.07837769389152527\n",
      "\n",
      "training loss at epoch 2 is 0.2724640965461731\n",
      "test_loss at epoch 2 is 0.0833655595779419\n",
      "\n",
      "training loss at epoch 3 is 0.008372262120246887\n",
      "test_loss at epoch 3 is 0.06718043982982635\n",
      "\n",
      "training loss at epoch 4 is 0.14380982518196106\n",
      "test_loss at epoch 4 is 0.047370411455631256\n",
      "\n",
      "training loss at epoch 5 is 0.13568785786628723\n",
      "pred = tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1]) \n",
      " label= tensor([1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1])\n",
      "\n",
      "test_loss at epoch 5 is 0.08299298584461212\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion( pred, labels)\n\u001B[0;32m     21\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 22\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m#print(f'training loss at epoch {i} is {loss.detach()}')\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# we train our stuff here now\n",
    "\n",
    "#run through all the epochs\n",
    "\n",
    "# for every epoch\n",
    "# get the train data from train_loader along with its label class\n",
    "# perform the model on it, get the loss by comparing with truth, and then get backward the loss\n",
    "# for every 10 epochs do the evaluation\n",
    "opt.lr = 1e-6\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 100\n",
    "for i in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    loss_epoch = []\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(images)\n",
    "        loss = criterion( pred, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        #print(f'training loss at epoch {i} is {loss.detach()}')\n",
    "        loss_epoch.append(loss.detach())\n",
    "    losses.append(sum(loss_epoch))\n",
    "    print(f'training loss at epoch {i} is {losses[-1]}')\n",
    "    if i%1 == 0:\n",
    "        # evaluate here.\n",
    "        model.eval()\n",
    "        for images, labels in test_loader:\n",
    "            images =images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images)\n",
    "            loss = criterion(pred, labels).detach()\n",
    "            if i%5 == 0 : print(f'pred = {pred.argmax(dim=1)} \\n label= {labels}\\n')\n",
    "            print(f'test_loss at epoch {i} is {loss}\\n')\n",
    "        model.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:23:45.855171100Z",
     "start_time": "2023-05-15T20:22:22.695474800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True, True, True, True, True,\n        True, True, True, True, True, True, True, True])"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:24:00.905352Z",
     "start_time": "2023-05-15T20:24:00.872026400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n        0, 0, 1, 1, 0, 1, 1, 1])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:18:26.218296500Z",
     "start_time": "2023-05-15T20:18:26.172997900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# We will train only the last layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            (9,408)\n",
      "├─BatchNorm2d: 1-2                       (128)\n",
      "├─ReLU: 1-3                              --\n",
      "├─MaxPool2d: 1-4                         --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─BasicBlock: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  (36,864)\n",
      "|    |    └─BatchNorm2d: 3-2             (128)\n",
      "|    |    └─ReLU: 3-3                    --\n",
      "|    |    └─Conv2d: 3-4                  (36,864)\n",
      "|    |    └─BatchNorm2d: 3-5             (128)\n",
      "|    └─BasicBlock: 2-2                   --\n",
      "|    |    └─Conv2d: 3-6                  (36,864)\n",
      "|    |    └─BatchNorm2d: 3-7             (128)\n",
      "|    |    └─ReLU: 3-8                    --\n",
      "|    |    └─Conv2d: 3-9                  (36,864)\n",
      "|    |    └─BatchNorm2d: 3-10            (128)\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─BasicBlock: 2-3                   --\n",
      "|    |    └─Conv2d: 3-11                 (73,728)\n",
      "|    |    └─BatchNorm2d: 3-12            (256)\n",
      "|    |    └─ReLU: 3-13                   --\n",
      "|    |    └─Conv2d: 3-14                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-15            (256)\n",
      "|    |    └─Sequential: 3-16             (8,448)\n",
      "|    └─BasicBlock: 2-4                   --\n",
      "|    |    └─Conv2d: 3-17                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-18            (256)\n",
      "|    |    └─ReLU: 3-19                   --\n",
      "|    |    └─Conv2d: 3-20                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-21            (256)\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─BasicBlock: 2-5                   --\n",
      "|    |    └─Conv2d: 3-22                 (294,912)\n",
      "|    |    └─BatchNorm2d: 3-23            (512)\n",
      "|    |    └─ReLU: 3-24                   --\n",
      "|    |    └─Conv2d: 3-25                 (589,824)\n",
      "|    |    └─BatchNorm2d: 3-26            (512)\n",
      "|    |    └─Sequential: 3-27             (33,280)\n",
      "|    └─BasicBlock: 2-6                   --\n",
      "|    |    └─Conv2d: 3-28                 (589,824)\n",
      "|    |    └─BatchNorm2d: 3-29            (512)\n",
      "|    |    └─ReLU: 3-30                   --\n",
      "|    |    └─Conv2d: 3-31                 (589,824)\n",
      "|    |    └─BatchNorm2d: 3-32            (512)\n",
      "├─Sequential: 1-8                        --\n",
      "|    └─BasicBlock: 2-7                   --\n",
      "|    |    └─Conv2d: 3-33                 (1,179,648)\n",
      "|    |    └─BatchNorm2d: 3-34            (1,024)\n",
      "|    |    └─ReLU: 3-35                   --\n",
      "|    |    └─Conv2d: 3-36                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-37            (1,024)\n",
      "|    |    └─Sequential: 3-38             (132,096)\n",
      "|    └─BasicBlock: 2-8                   --\n",
      "|    |    └─Conv2d: 3-39                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-40            (1,024)\n",
      "|    |    └─ReLU: 3-41                   --\n",
      "|    |    └─Conv2d: 3-42                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-43            (1,024)\n",
      "├─AdaptiveAvgPool2d: 1-9                 --\n",
      "├─Linear: 1-10                           1,026\n",
      "=================================================================\n",
      "Total params: 11,177,538\n",
      "Trainable params: 1,026\n",
      "Non-trainable params: 11,176,512\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Conv2d: 1-1                            (9,408)\n├─BatchNorm2d: 1-2                       (128)\n├─ReLU: 1-3                              --\n├─MaxPool2d: 1-4                         --\n├─Sequential: 1-5                        --\n|    └─BasicBlock: 2-1                   --\n|    |    └─Conv2d: 3-1                  (36,864)\n|    |    └─BatchNorm2d: 3-2             (128)\n|    |    └─ReLU: 3-3                    --\n|    |    └─Conv2d: 3-4                  (36,864)\n|    |    └─BatchNorm2d: 3-5             (128)\n|    └─BasicBlock: 2-2                   --\n|    |    └─Conv2d: 3-6                  (36,864)\n|    |    └─BatchNorm2d: 3-7             (128)\n|    |    └─ReLU: 3-8                    --\n|    |    └─Conv2d: 3-9                  (36,864)\n|    |    └─BatchNorm2d: 3-10            (128)\n├─Sequential: 1-6                        --\n|    └─BasicBlock: 2-3                   --\n|    |    └─Conv2d: 3-11                 (73,728)\n|    |    └─BatchNorm2d: 3-12            (256)\n|    |    └─ReLU: 3-13                   --\n|    |    └─Conv2d: 3-14                 (147,456)\n|    |    └─BatchNorm2d: 3-15            (256)\n|    |    └─Sequential: 3-16             (8,448)\n|    └─BasicBlock: 2-4                   --\n|    |    └─Conv2d: 3-17                 (147,456)\n|    |    └─BatchNorm2d: 3-18            (256)\n|    |    └─ReLU: 3-19                   --\n|    |    └─Conv2d: 3-20                 (147,456)\n|    |    └─BatchNorm2d: 3-21            (256)\n├─Sequential: 1-7                        --\n|    └─BasicBlock: 2-5                   --\n|    |    └─Conv2d: 3-22                 (294,912)\n|    |    └─BatchNorm2d: 3-23            (512)\n|    |    └─ReLU: 3-24                   --\n|    |    └─Conv2d: 3-25                 (589,824)\n|    |    └─BatchNorm2d: 3-26            (512)\n|    |    └─Sequential: 3-27             (33,280)\n|    └─BasicBlock: 2-6                   --\n|    |    └─Conv2d: 3-28                 (589,824)\n|    |    └─BatchNorm2d: 3-29            (512)\n|    |    └─ReLU: 3-30                   --\n|    |    └─Conv2d: 3-31                 (589,824)\n|    |    └─BatchNorm2d: 3-32            (512)\n├─Sequential: 1-8                        --\n|    └─BasicBlock: 2-7                   --\n|    |    └─Conv2d: 3-33                 (1,179,648)\n|    |    └─BatchNorm2d: 3-34            (1,024)\n|    |    └─ReLU: 3-35                   --\n|    |    └─Conv2d: 3-36                 (2,359,296)\n|    |    └─BatchNorm2d: 3-37            (1,024)\n|    |    └─Sequential: 3-38             (132,096)\n|    └─BasicBlock: 2-8                   --\n|    |    └─Conv2d: 3-39                 (2,359,296)\n|    |    └─BatchNorm2d: 3-40            (1,024)\n|    |    └─ReLU: 3-41                   --\n|    |    └─Conv2d: 3-42                 (2,359,296)\n|    |    └─BatchNorm2d: 3-43            (1,024)\n├─AdaptiveAvgPool2d: 1-9                 --\n├─Linear: 1-10                           1,026\n=================================================================\nTotal params: 11,177,538\nTrainable params: 1,026\nNon-trainable params: 11,176,512\n================================================================="
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_classes = 2\n",
    "model = models.resnet18(pretrained= True)\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features , out_features=num_classes)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.fc.requires_grad_(True)\n",
    "\n",
    "torchsummary.summary(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:32:59.434588900Z",
     "start_time": "2023-05-15T20:32:58.893329500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "opt = Adam(model.parameters() , lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:32:59.506979900Z",
     "start_time": "2023-05-15T20:32:59.443468600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at epoch 0 is 0.18788492679595947\n",
      "pred = tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]) \n",
      " label= tensor([0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1])\n",
      " true accuracy = 1.0\n",
      "test_loss at epoch 0 is 0.21542128920555115\n",
      "\n",
      "training loss at epoch 1 is 0.20445393025875092\n",
      "test_loss at epoch 1 is 0.22265474498271942\n",
      "\n",
      "training loss at epoch 2 is 0.1745493859052658\n",
      "test_loss at epoch 2 is 0.22470995783805847\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[32], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m images \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     20\u001B[0m labels \u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 21\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion( pred, labels)\n\u001B[0;32m     23\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001B[0m, in \u001B[0;36mResNet.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:274\u001B[0m, in \u001B[0;36mResNet._forward_impl\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    271\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmaxpool(x)\n\u001B[0;32m    273\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer1(x)\n\u001B[1;32m--> 274\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    275\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n\u001B[0;32m    276\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer4(x)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\resnet.py:100\u001B[0m, in \u001B[0;36mBasicBlock.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     97\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn2(out)\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdownsample \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 100\u001B[0m     identity \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownsample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    102\u001B[0m out \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m identity\n\u001B[0;32m    103\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelu(out)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 217\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    164\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    166\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[0;32m    169\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[0;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[0;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    183\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001B[0m, in \u001B[0;36mbatch_norm\u001B[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[0;32m   2447\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[0;32m   2448\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[1;32m-> 2450\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2451\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[0;32m   2452\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# we train our stuff here now\n",
    "\n",
    "#run through all the epochs\n",
    "\n",
    "# for every epoch\n",
    "# get the train data from train_loader along with its label class\n",
    "# perform the model on it, get the loss by comparing with truth, and then get backward the loss\n",
    "# for every 10 epochs do the evaluation\n",
    "opt.lr = 1e-6\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 100\n",
    "for i in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    loss_epoch = []\n",
    "    count =0\n",
    "    for images, labels in train_loader:\n",
    "        count += 1\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(images)\n",
    "        loss = criterion( pred, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        #print(f'training loss at epoch {i} is {loss.detach()}')\n",
    "        loss_epoch.append(loss.detach())\n",
    "    losses.append(sum(loss_epoch)/count)\n",
    "    print(f'training loss at epoch {i} is {losses[-1]}')\n",
    "    if i%1 == 0:\n",
    "        # evaluate here.\n",
    "        model.eval()\n",
    "        for images, labels in test_loader:\n",
    "            images =images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images)\n",
    "            loss = criterion(pred, labels).detach()\n",
    "            if i%5 == 0 : print(f'pred = {pred.argmax(dim=1)} \\n label= {labels}\\n true accuracy = {sum(pred.argmax(dim=1) == labels)/len(labels)}')\n",
    "            print(f'test_loss at epoch {i} is {loss}\\n')\n",
    "        model.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:42:12.739402100Z",
     "start_time": "2023-05-15T20:41:51.173473300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now we will train model initialized from completely random weights"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dushyant S. Udawat\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            9,408\n",
      "├─BatchNorm2d: 1-2                       128\n",
      "├─ReLU: 1-3                              --\n",
      "├─MaxPool2d: 1-4                         --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─BasicBlock: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  36,864\n",
      "|    |    └─BatchNorm2d: 3-2             128\n",
      "|    |    └─ReLU: 3-3                    --\n",
      "|    |    └─Conv2d: 3-4                  36,864\n",
      "|    |    └─BatchNorm2d: 3-5             128\n",
      "|    └─BasicBlock: 2-2                   --\n",
      "|    |    └─Conv2d: 3-6                  36,864\n",
      "|    |    └─BatchNorm2d: 3-7             128\n",
      "|    |    └─ReLU: 3-8                    --\n",
      "|    |    └─Conv2d: 3-9                  36,864\n",
      "|    |    └─BatchNorm2d: 3-10            128\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─BasicBlock: 2-3                   --\n",
      "|    |    └─Conv2d: 3-11                 73,728\n",
      "|    |    └─BatchNorm2d: 3-12            256\n",
      "|    |    └─ReLU: 3-13                   --\n",
      "|    |    └─Conv2d: 3-14                 147,456\n",
      "|    |    └─BatchNorm2d: 3-15            256\n",
      "|    |    └─Sequential: 3-16             8,448\n",
      "|    └─BasicBlock: 2-4                   --\n",
      "|    |    └─Conv2d: 3-17                 147,456\n",
      "|    |    └─BatchNorm2d: 3-18            256\n",
      "|    |    └─ReLU: 3-19                   --\n",
      "|    |    └─Conv2d: 3-20                 147,456\n",
      "|    |    └─BatchNorm2d: 3-21            256\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─BasicBlock: 2-5                   --\n",
      "|    |    └─Conv2d: 3-22                 294,912\n",
      "|    |    └─BatchNorm2d: 3-23            512\n",
      "|    |    └─ReLU: 3-24                   --\n",
      "|    |    └─Conv2d: 3-25                 589,824\n",
      "|    |    └─BatchNorm2d: 3-26            512\n",
      "|    |    └─Sequential: 3-27             33,280\n",
      "|    └─BasicBlock: 2-6                   --\n",
      "|    |    └─Conv2d: 3-28                 589,824\n",
      "|    |    └─BatchNorm2d: 3-29            512\n",
      "|    |    └─ReLU: 3-30                   --\n",
      "|    |    └─Conv2d: 3-31                 589,824\n",
      "|    |    └─BatchNorm2d: 3-32            512\n",
      "├─Sequential: 1-8                        --\n",
      "|    └─BasicBlock: 2-7                   --\n",
      "|    |    └─Conv2d: 3-33                 1,179,648\n",
      "|    |    └─BatchNorm2d: 3-34            1,024\n",
      "|    |    └─ReLU: 3-35                   --\n",
      "|    |    └─Conv2d: 3-36                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-37            1,024\n",
      "|    |    └─Sequential: 3-38             132,096\n",
      "|    └─BasicBlock: 2-8                   --\n",
      "|    |    └─Conv2d: 3-39                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-40            1,024\n",
      "|    |    └─ReLU: 3-41                   --\n",
      "|    |    └─Conv2d: 3-42                 2,359,296\n",
      "|    |    └─BatchNorm2d: 3-43            1,024\n",
      "├─AdaptiveAvgPool2d: 1-9                 --\n",
      "├─Linear: 1-10                           1,026\n",
      "=================================================================\n",
      "Total params: 11,177,538\n",
      "Trainable params: 11,177,538\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "=================================================================\nLayer (type:depth-idx)                   Param #\n=================================================================\n├─Conv2d: 1-1                            9,408\n├─BatchNorm2d: 1-2                       128\n├─ReLU: 1-3                              --\n├─MaxPool2d: 1-4                         --\n├─Sequential: 1-5                        --\n|    └─BasicBlock: 2-1                   --\n|    |    └─Conv2d: 3-1                  36,864\n|    |    └─BatchNorm2d: 3-2             128\n|    |    └─ReLU: 3-3                    --\n|    |    └─Conv2d: 3-4                  36,864\n|    |    └─BatchNorm2d: 3-5             128\n|    └─BasicBlock: 2-2                   --\n|    |    └─Conv2d: 3-6                  36,864\n|    |    └─BatchNorm2d: 3-7             128\n|    |    └─ReLU: 3-8                    --\n|    |    └─Conv2d: 3-9                  36,864\n|    |    └─BatchNorm2d: 3-10            128\n├─Sequential: 1-6                        --\n|    └─BasicBlock: 2-3                   --\n|    |    └─Conv2d: 3-11                 73,728\n|    |    └─BatchNorm2d: 3-12            256\n|    |    └─ReLU: 3-13                   --\n|    |    └─Conv2d: 3-14                 147,456\n|    |    └─BatchNorm2d: 3-15            256\n|    |    └─Sequential: 3-16             8,448\n|    └─BasicBlock: 2-4                   --\n|    |    └─Conv2d: 3-17                 147,456\n|    |    └─BatchNorm2d: 3-18            256\n|    |    └─ReLU: 3-19                   --\n|    |    └─Conv2d: 3-20                 147,456\n|    |    └─BatchNorm2d: 3-21            256\n├─Sequential: 1-7                        --\n|    └─BasicBlock: 2-5                   --\n|    |    └─Conv2d: 3-22                 294,912\n|    |    └─BatchNorm2d: 3-23            512\n|    |    └─ReLU: 3-24                   --\n|    |    └─Conv2d: 3-25                 589,824\n|    |    └─BatchNorm2d: 3-26            512\n|    |    └─Sequential: 3-27             33,280\n|    └─BasicBlock: 2-6                   --\n|    |    └─Conv2d: 3-28                 589,824\n|    |    └─BatchNorm2d: 3-29            512\n|    |    └─ReLU: 3-30                   --\n|    |    └─Conv2d: 3-31                 589,824\n|    |    └─BatchNorm2d: 3-32            512\n├─Sequential: 1-8                        --\n|    └─BasicBlock: 2-7                   --\n|    |    └─Conv2d: 3-33                 1,179,648\n|    |    └─BatchNorm2d: 3-34            1,024\n|    |    └─ReLU: 3-35                   --\n|    |    └─Conv2d: 3-36                 2,359,296\n|    |    └─BatchNorm2d: 3-37            1,024\n|    |    └─Sequential: 3-38             132,096\n|    └─BasicBlock: 2-8                   --\n|    |    └─Conv2d: 3-39                 2,359,296\n|    |    └─BatchNorm2d: 3-40            1,024\n|    |    └─ReLU: 3-41                   --\n|    |    └─Conv2d: 3-42                 2,359,296\n|    |    └─BatchNorm2d: 3-43            1,024\n├─AdaptiveAvgPool2d: 1-9                 --\n├─Linear: 1-10                           1,026\n=================================================================\nTotal params: 11,177,538\nTrainable params: 11,177,538\nNon-trainable params: 0\n================================================================="
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_classes = 2\n",
    "model = models.resnet18(pretrained= False)\n",
    "model.fc = nn.Linear(in_features=model.fc.in_features , out_features=num_classes)\n",
    "\n",
    "torchsummary.summary(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:42:15.660955300Z",
     "start_time": "2023-05-15T20:42:15.304594400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "opt = Adam(model.parameters() , lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:42:17.652749900Z",
     "start_time": "2023-05-15T20:42:17.621494500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at epoch 0 is 0.4685856103897095\n",
      "pred = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \n",
      " label= tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1])\n",
      " true accuracy = 0.5\n",
      "test_loss at epoch 0 is 0.741663932800293\n",
      "\n",
      "training loss at epoch 1 is 0.6134694218635559\n",
      "test_loss at epoch 1 is 1.1679657697677612\n",
      "\n",
      "training loss at epoch 2 is 0.3706599175930023\n",
      "test_loss at epoch 2 is 1.366943597793579\n",
      "\n",
      "training loss at epoch 3 is 0.19305920600891113\n",
      "test_loss at epoch 3 is 1.4533299207687378\n",
      "\n",
      "training loss at epoch 4 is 0.1489115208387375\n",
      "test_loss at epoch 4 is 1.7867956161499023\n",
      "\n",
      "training loss at epoch 5 is 0.04631601274013519\n",
      "pred = tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) \n",
      " label= tensor([1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0])\n",
      " true accuracy = 0.5\n",
      "test_loss at epoch 5 is 2.398198127746582\n",
      "\n",
      "training loss at epoch 6 is 0.05309541895985603\n",
      "test_loss at epoch 6 is 3.2148635387420654\n",
      "\n",
      "training loss at epoch 7 is 0.022992299869656563\n",
      "test_loss at epoch 7 is 3.662889003753662\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 24\u001B[0m\n\u001B[0;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion( pred, labels)\n\u001B[0;32m     23\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 24\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     25\u001B[0m opt\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m#print(f'training loss at epoch {i} is {loss.detach()}')\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    478\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    479\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    480\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    485\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    486\u001B[0m     )\n\u001B[1;32m--> 487\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    195\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    199\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 200\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    201\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# we train our stuff here now\n",
    "\n",
    "#run through all the epochs\n",
    "\n",
    "# for every epoch\n",
    "# get the train data from train_loader along with its label class\n",
    "# perform the model on it, get the loss by comparing with truth, and then get backward the loss\n",
    "# for every 10 epochs do the evaluation\n",
    "opt.lr = 1e-6\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_epochs = 100\n",
    "for i in range(num_epochs):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    loss_epoch = []\n",
    "    count =0\n",
    "    for images, labels in train_loader:\n",
    "        count += 1\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        pred = model(images)\n",
    "        loss = criterion( pred, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        #print(f'training loss at epoch {i} is {loss.detach()}')\n",
    "        loss_epoch.append(loss.detach())\n",
    "    losses.append(sum(loss_epoch)/count)\n",
    "    print(f'training loss at epoch {i} is {losses[-1]}')\n",
    "    if i%1 == 0:\n",
    "        # evaluate here.\n",
    "        model.eval()\n",
    "        for images, labels in test_loader:\n",
    "            images =images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pred = model(images)\n",
    "            loss = criterion(pred, labels).detach()\n",
    "            if i%5 == 0 : print(f'pred = {pred.argmax(dim=1)} \\n label= {labels}\\n true accuracy = {sum(pred.argmax(dim=1) == labels)/len(labels)}')\n",
    "            print(f'test_loss at epoch {i} is {loss}\\n')\n",
    "        model.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T20:44:41.895936500Z",
     "start_time": "2023-05-15T20:42:41.244590500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(num_epochs) , losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
